# **Lecture notes of Multiple linear regression**

<img width="433" alt="image" src="https://github.com/Nikhila-KS/Unravel_ML/assets/100426366/8b600a73-9f8b-4420-9b4d-870645e0daa0">

These four data sets are called Anscombe's quartet

and they illustrate that you can't just simply,

blindly apply linear regression,

you have to make sure that your dataset

is fit for using linear regression.

And that's where assumptions of linear regression come in.

So let's have a look at them.

There are going to be five assumptions in total

plus an extra check.

<img width="653" alt="image" src="https://github.com/Nikhila-KS/Unravel_ML/assets/100426366/6613b20a-da8f-4692-82bd-fce8f86200c7">

The first assumption is linearity.

We want to make sure that there is a linear relationship

between our dependent variable

and each independent variable.

And if you look at the chart here on the right,

you'll see that the linear regression is misleading.

There is actually no linear relationship

between the two variables.

So we wouldn't use this kind of model there.

The second assumption is homoscedasticity.

And even though it sounds like a complex term,

it actually simply means equal variance.

Meaning that you don't want to see a cone type shape

on your chart whether an increasing cone

or a decreasing cone.

Which would mean that variance is dependent

on the independent variable.

So in this case, we wouldn't use a linear regression either.

The third assumption is multivariate normality

or normality of error distribution.

If you look at the chart here on the right,

you can feel that something is off.

The best way to intuitively think about it

is if you look along the line of the linear regression,

you want to see a normal distribution of your data point.

In the case on the right here,

we can see something different.

And so, again, we wouldn't apply a linear regression there.

The fourth assumption is independence of observations.

And this includes the term no autocorrelation.

Sometimes you'll see the assumption

titled as no autocorrelation.

And what that means is that we don't want to see

any kind of pattern in our data.

A pattern in the data, like we see here,

indicates that our rows are not independent,

that some rows are affecting other rows

and other rows, et cetera.

A classic example of this would be the stock market

where previous prices affect future prices,

which affect future prices and so on.

So in this case,

we wouldn't apply a linear regression model.

The fifth assumption is lack of multicollinearity.

Basically, we want our independent variables

or predictors not to be correlated with each other.

If they're not correlated,

then we can build a linear regression.

If they are correlated,

then if we do proceed and build a linear regression model,

then the coefficient estimates that we get in the model

will become unreliable.

And the sixth point is the outlier check.

This is not an actual assumption but rather an extra check

that is important to keep in mind

when building linear regression models.

If you look at the chart here on the right,

you can see that the outlier is significantly affecting

the linear regression line that we get.

So something that we want to consider is,

should we remove the outliers

before building a linear regression

or do we want to build a linear regression

with the outliers included?

This will depend on your business knowledge

and knowledge of the data set.

**the approach that you need to take when you face categorical variables in** 
**regression models is you need to create dummy variables.**
<img width="455" alt="image" src="https://github.com/Nikhila-KS/Unravel_ML/assets/100426366/6cefba03-2df8-4c5e-95f6-c9cc1ab9399c">
