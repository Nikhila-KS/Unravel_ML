{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhila-KS/Unravel_ML/blob/main/4.)Understanding_YAMNet_myNotes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk5u_9KN1m-t"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/yamnet\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/yamnet.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/hub/blob/master/examples/colab/yamnet.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/hub/examples/colab/yamnet.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://tfhub.dev/google/yamnet/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2ep-q7k_5R-"
      },
      "source": [
        "# Sound classification with YAMNet\n",
        "----------------------------------\n",
        "\n",
        "YAMNet is a deep net that predicts 521 audio event [classes](https://github.com/tensorflow/models/blob/master/research/audioset/yamnet/yamnet_class_map.csv) from the [AudioSet-YouTube corpus](http://g.co/audioset) it was trained on. It employs the\n",
        "[Mobilenet_v1](https://arxiv.org/pdf/1704.04861.pdf) depthwise-separable\n",
        "convolution architecture.\n",
        "\n",
        "In Simple words -\n",
        "\n",
        "YAMNet is a pre-trained deep neural network that can predict audio events from 521 classes, such as laughter, barking, or a siren. It was developed by Google AI and is available on TensorFlow Hub.\n",
        "\n",
        "YAMNet is based on the MobileNetV1 depthwise-separable convolution architecture, which is designed to be efficient and accurate at classifying audio events. It is trained on a large dataset of audio recordings called the AudioSet corpus, which contains over 2 million audio clips from YouTube videos.\n",
        "\n",
        "YAMNet can be used for a variety of tasks, such as:\n",
        "\n",
        "* Sound event detection: YAMNet can be used to automatically detect the sound events that are present in an audio recording. This can be useful for tasks such as creating transcripts of audio recordings or identifying the source of a noise complaint.\n",
        "* Audio tagging: YAMNet can be used to automatically tag audio recordings with the corresponding sound events. This can be useful for tasks such as organizing audio files or creating playlists.\n",
        "* Sound synthesis: YAMNet can be used to synthesize new audio recordings that contain specific sound events. This can be useful for tasks such as creating sound effects or generating music."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bteu7pfkpt_f"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "from scipy.io import wavfile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definitions**\n",
        "\n",
        "----------------------------------------\n",
        "**TensorFlow** is a popular open-source software library used for building and training machine learning models. It provides a framework that simplifies the process of developing and deploying machine learning algorithms. TensorFlow allows users to define and manipulate mathematical operations with multidimensional arrays called tensors, which are the fundamental building blocks of models.\n",
        "\n",
        "-------------------------------------------\n",
        "**TensorFlow Hub**, on the other hand, is an extension of TensorFlow that offers a repository of pre-trained machine learning models. These models are created by experts and can be easily reused in other projects. TensorFlow Hub simplifies the process of integrating pre-trained models into new applications, enabling developers to leverage existing models and their learned knowledge for specific tasks without having to start from scratch. It provides a convenient way to access and incorporate state-of-the-art models into your own machine learning projects.\n",
        "\n",
        "---------------------------------------------------\n",
        "**NumPy** is a popular Python library that stands for \"Numerical Python.\" It provides a powerful set of tools and functions for working with arrays and matrices, making it an essential library for scientific computing and data analysis, particularly in machine learning. NumPy allows you to perform various mathematical operations on arrays efficiently, such as mathematical calculations, linear algebra, random number generation, and reshaping data. It provides a convenient and optimized way to handle large datasets and perform computations on them.\n",
        "\n",
        "-------------------------------------------------------\n",
        "**CSV** which stands for \"Comma-Separated Values,\" is a simple file format used to store tabular data, such as spreadsheets or databases. It represents data in plain text, where each line of the file corresponds to a row of data, and the values within a row are separated by commas. CSV files are commonly used in machine learning for storing and importing datasets.\n",
        "\n",
        "-------------------------------------------------------\n",
        "**matplotlib.pyplot** is a module within the matplotlib library that provides a collection of functions for creating visualizations, such as plots, charts, and graphs, in Python. It allows you to display and customize data in a visually appealing manner.\n",
        "\n",
        "-----------------------------------\n",
        "\n",
        "**IPython.display** is a module that provides a set of functions to enhance the display of output within the IPython environment or Jupyter Notebook. It offers various capabilities to render and format different types of content, such as images, videos, audio, HTML, Markdown etc.\n",
        "\n",
        "--------------------------------------------\n",
        "**scipy.io** is a module within the SciPy library that provides functions for reading and writing data from different file formats. It enables you to load and save data in various formats, including MATLAB files, NetCDF files, WAV files, and more. The scipy.io module allows you to read data stored in these formats into a format that can be easily used within your Python code. It provides convenient functions to access and manipulate data from these file formats, enabling you to work with different types of data efficiently in your scientific or machine learning applications.\n",
        "\n",
        "----------------------------"
      ],
      "metadata": {
        "id": "_N1K8nLiQUAZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSVs3zRrrYmY"
      },
      "source": [
        "### Load the Model from TensorFlow Hub.\n",
        "\n",
        "Note: to read the documentation just follow the model's [url](https://tfhub.dev/google/yamnet/1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX8Vzs6EpwMo"
      },
      "outputs": [],
      "source": [
        "# Load the model.\n",
        "model = hub.load('https://tfhub.dev/google/yamnet/1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxWx6tOdtdBP"
      },
      "source": [
        "The labels file will be loaded from the models assets and is present at `model.class_map_path()`.\n",
        "You will load it on the `class_names` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHSToAW--o4U"
      },
      "outputs": [],
      "source": [
        "# Find the name of the class with the top score when mean-aggregated across frames.\n",
        "def class_names_from_csv(class_map_csv_text):\n",
        "  \"\"\"Returns list of class names corresponding to score vector.\"\"\"\n",
        "  class_names = []\n",
        "  with tf.io.gfile.GFile(class_map_csv_text) as csvfile:\n",
        "    #tf.io.gfile.GFile(class_map_csv_text) is a function call in TensorFlow that creates a file object for reading the contents\n",
        "    #of the specified file, class_map_csv_text. It is part of the tf.io.gfile module, which provides a file system interface\n",
        "    #compatible with multiple storage systems, such as local files, Google Cloud Storage, and\n",
        "    #Hadoop Distributed File System (HDFS).\n",
        "\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    print(\"the rows -\")\n",
        "    for row in reader:\n",
        "      class_names.append(row['display_name'])\n",
        "      print(row)\n",
        "  print(\"==============================================\")\n",
        "  print(\"list class_names\")\n",
        "  print(class_names)\n",
        "  return class_names\n",
        "\n",
        "class_map_path = model.class_map_path().numpy()\n",
        "class_names = class_names_from_csv(class_map_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSFjRwkZ59lU"
      },
      "source": [
        "Add a method to verify and convert a loaded audio is on the proper sample_rate (16K), otherwise it would affect the model's results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LizGwWjc5w6A"
      },
      "outputs": [],
      "source": [
        "def ensure_sample_rate(original_sample_rate, waveform,\n",
        "                       desired_sample_rate=16000):\n",
        "  \"\"\"Resample waveform if required.\"\"\"\n",
        "  if original_sample_rate != desired_sample_rate:\n",
        "    desired_length = int(round(float(len(waveform)) /\n",
        "                               original_sample_rate * desired_sample_rate))\n",
        "    waveform = scipy.signal.resample(waveform, desired_length)\n",
        "  return desired_sample_rate, waveform"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes three arguments:\n",
        "\n",
        "1] original_sample_rate: The original sample rate of the waveform.\n",
        "\n",
        "2] waveform: The waveform data.\n",
        "\n",
        "3] desired_sample_rate: The desired sample rate of the waveform.\n",
        "The function first checks if the original sample rate is different from the desired sample rate. If it is, then the function resamples the waveform to the desired sample rate. Resampling is the process of converting a waveform from one sample rate to another.\n",
        "\n",
        "The function then returns the desired sample rate and the resampled waveform.\n",
        "\n",
        "key concepts:\n",
        "\n",
        "Sample rate:  Sample rate is a measurement of the number of samples we take per second of audio; and therefore the speed at which we do so. In other words Sample rate is the number of samples of audio carried per second, measured in Hz or kHz\n",
        "\n",
        "Waveform: A waveform is a graphical representation of a sound wave.\n",
        "\n",
        "Resample: To resample a waveform is to convert it from one sample rate to another."
      ],
      "metadata": {
        "id": "xli6MnOHoyKc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZEgCobA9bWl"
      },
      "source": [
        "## Downloading and preparing the sound file\n",
        "\n",
        "Here you will download a wav file and listen to it.\n",
        "If you have a file already available, just upload it to colab and use it instead.\n",
        "\n",
        "Note: The expected audio file should be a mono wav file at 16kHz sample rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzZHvyTtsJrc"
      },
      "outputs": [],
      "source": [
        "!curl -O https://storage.googleapis.com/audioset/speech_whistling2.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8LKmqvGzZzr"
      },
      "outputs": [],
      "source": [
        "!curl -O https://storage.googleapis.com/audioset/miaow_16k.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo9KJb-5zuz1"
      },
      "outputs": [],
      "source": [
        "# wav_file_name = 'speech_whistling2.wav'\n",
        "wav_file_name = 'miaow_16k.wav'\n",
        "sample_rate, wav_data = wavfile.read(wav_file_name, 'rb')\n",
        "sample_rate, wav_data = ensure_sample_rate(sample_rate, wav_data)\n",
        "\n",
        "'''\n",
        "This line of code uses the wavfile module to read the WAV file and store the following information in two variables:\n",
        "sample_rate: The sample rate of the WAV file, which is the number of times per second that the audio signal is sampled.\n",
        "wav_data: The audio data from the WAV file, which is a NumPy array of numbers representing the amplitude of the audio signal at\n",
        "each sample.This line of code calls the ensure_sample_rate function to ensure that the sample rate of the WAV file is 16,000 Hz.\n",
        "This is the standard sample rate for audio files.\n",
        "'''\n",
        "\n",
        "# Show some basic information about the audio.\n",
        "duration = len(wav_data)/sample_rate\n",
        "print(f'Sample rate: {sample_rate} Hz')\n",
        "print(f'Total duration: {duration:.2f}s')\n",
        "print(f'Size of the input: {len(wav_data)}')\n",
        "\n",
        "# Listening to the wav file.\n",
        "print(' ')\n",
        "Audio(wav_data, rate=sample_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9I290COsMBm"
      },
      "source": [
        "The `wav_data` needs to be normalized to values in `[-1.0, 1.0]` (as stated in the model's [documentation](https://tfhub.dev/google/yamnet/1))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKr78aCBsQo3"
      },
      "outputs": [],
      "source": [
        "waveform = wav_data / tf.int16.max"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_Xwd4GPuMsB"
      },
      "source": [
        "## Executing the Model\n",
        "\n",
        "Now the easy part: using the data already prepared, you just call the model and get the: scores, embedding and the spectrogram.\n",
        "\n",
        "The score is the main result you will use.\n",
        "The spectrogram you will use to do some visualizations later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJGP6r-At_Jc"
      },
      "outputs": [],
      "source": [
        "# Run the model, check the output.\n",
        "scores, embeddings, spectrogram = model(waveform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vmo7griQprDk"
      },
      "outputs": [],
      "source": [
        "scores_np = scores.numpy()\n",
        "print('scores array - \\n',scores_np)\n",
        "spectrogram_np = spectrogram.numpy()\n",
        "infered_class = class_names[scores_np.mean(axis=0).argmax()]\n",
        "print('\\n The main sound is: {infered_class}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj2xLf-P_ndS"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "YAMNet also returns some additional information that we can use for visualization.\n",
        "Let's take a look on the Waveform, spectrogram and the top classes inferred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QSTkmv7wr2M"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the waveform.\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(waveform)\n",
        "plt.xlim([0, len(waveform)])\n",
        "\n",
        "# Plot the log-mel spectrogram (returned by the model).\n",
        "'''A log-mel spectrogram is a visual representation of the frequency content of a sound. It is created by taking the Fourier\n",
        "transform of a sound signal, and then applying a logarithm and a mel filterbank to the resulting frequency spectrum.'''\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.imshow(spectrogram_np.T, aspect='auto', interpolation='nearest', origin='lower')\n",
        "\n",
        "# Plot and label the model output scores for the top-scoring classes.\n",
        "mean_scores = np.mean(scores, axis=0)\n",
        "top_n = 10\n",
        "top_class_indices = np.argsort(mean_scores)[::-1][:top_n]\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.imshow(scores_np[:, top_class_indices].T, aspect='auto', interpolation='nearest', cmap='gray_r')\n",
        "\n",
        "# patch_padding = (PATCH_WINDOW_SECONDS / 2) / PATCH_HOP_SECONDS\n",
        "# values from the model documentation\n",
        "patch_padding = (0.025 / 2) / 0.01\n",
        "plt.xlim([-patch_padding-0.5, scores.shape[0] + patch_padding-0.5])\n",
        "# Label the top_N classes.\n",
        "yticks = range(0, top_n, 1)\n",
        "plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks])\n",
        "_ = plt.ylim(-0.5 + np.array([top_n, 0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of above code by BARD -\n",
        "\n",
        "\n",
        "The code you provided is used to plot three different visualizations of a sound: the waveform, the log-mel spectrogram, and the model output scores for the top-scoring classes.\n",
        "\n",
        "Code snippet\n",
        "```\n",
        "plt.figure(figsize=(10, 6))\n",
        "```\n",
        "\n",
        "This line of code creates a new figure with a width of 10 inches and a height of 6 inches.\n",
        "\n",
        "Code snippet\n",
        "```\n",
        "# Plot the waveform.\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(waveform)\n",
        "plt.xlim([0, len(waveform)])\n",
        "```\n",
        "\n",
        "This code creates a new subplot in the figure and plots the waveform. The subplot is divided into 3 rows and 1 column, and this code is creating the first subplot. The waveform is plotted using the plot() function, and the x-axis is limited to the range of values from 0 to the length of the waveform.\n",
        "\n",
        "Code snippet\n",
        "```\n",
        "# Plot the log-mel spectrogram (returned by the model).\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.imshow(spectrogram_np.T, aspect='auto', interpolation='nearest', origin='lower')\n",
        "```\n",
        "\n",
        "This code creates a new subplot in the figure and plots the log-mel spectrogram. The subplot is divided into 3 rows and 1 column, and this code is creating the second subplot. The log-mel spectrogram is plotted using the imshow() function. The aspect argument is set to auto to automatically scale the axes so that the spectrogram is displayed as a square. The interpolation argument is set to nearest to use nearest-neighbor interpolation when displaying the spectrogram. The origin argument is set to lower to display the spectrogram with the origin at the bottom-left corner.\n",
        "\n",
        "Code snippet\n",
        "```\n",
        "# Plot and label the model output scores for the top-scoring classes.\n",
        "mean_scores = np.mean(scores, axis=0)\n",
        "top_n = 10\n",
        "top_class_indices = np.argsort(mean_scores)[::-1][:top_n]\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.imshow(scores_np[:, top_class_indices].T, aspect='auto', interpolation='nearest', cmap='gray_r')\n",
        "\n",
        "# patch_padding = (PATCH_WINDOW_SECONDS / 2) / PATCH_HOP_SECONDS\n",
        "# values from the model documentation\n",
        "patch_padding = (0.025 / 2) / 0.01\n",
        "plt.xlim([-patch_padding-0.5, scores.shape[0] + patch_padding-0.5])\n",
        "# Label the top_N classes.\n",
        "yticks = range(0, top_n, 1)\n",
        "plt.yticks(yticks, [class_names[top_class_indices[x]] for x in yticks])\n",
        "_ = plt.ylim(-0.5 + np.array([top_n, 0]))\n",
        "```\n",
        "\n",
        "This code creates a new subplot in the figure and plots the model output scores for the top-scoring classes. The subplot is divided into 3 rows and 1 column, and this code is creating the third subplot. The model output scores are plotted using the imshow() function. The aspect argument is set to auto to automatically scale the axes so that the scores are displayed as a square. The interpolation argument is set to nearest to use nearest-neighbor interpolation when displaying the scores. The cmap argument is set to gray_r to display the scores in a grayscale colormap.\n",
        "\n",
        "The patch_padding variable is calculated as the half-width of the patch window divided by the patch hop size. This value is used to set the x-limits of the subplot so that the patches are centered in the subplot.\n",
        "\n",
        "The yticks variable is created to hold the labels for the top-N classes. The labels are then plotted using the yticks() function. The _ variable is used to suppress the output of the ylim() function.\n",
        "\n",
        "---------------------------------\n",
        "The code you provided creates a figure with three subplots. The first subplot plots the waveform of a sound. The second subplot plots the log-mel spectrogram of the sound. The third subplot plots the model output scores for the top-scoring classes.\n",
        "\n",
        "The waveform is a graph of the sound's amplitude over time. The amplitude is a measure of the loudness of the sound.\n",
        "\n",
        "The log-mel spectrogram is a graph of the sound's frequency content over time. The frequency content is a measure of the different pitches that are present in the sound.\n",
        "\n",
        "The model output scores are a measure of how likely it is that the sound belongs to each of the different classes.\n",
        "\n",
        "\n",
        "Here are some of the things you can learn from the plots:\n",
        "\n",
        "* The waveform can tell you about the overall structure of the sound. For example, you can see that the sound in the example code has a regular beat, which suggests that it is a piece of music.\n",
        "* The log-mel spectrogram can tell you about the different frequencies that are present in the sound. For example, you can see that the sound in the example code has a lot of high-frequency content, which suggests that it is a high-pitched sound.\n",
        "* The model output scores can tell you which classes the sound is most likely to belong to. For example, the sound in the example code has the highest scores for the classes \"piano\" and \"guitar,\" which suggests that it is a piece of music that features both instruments."
      ],
      "metadata": {
        "id": "p_lXswSnv8Dm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "AZEgCobA9bWl"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}